# -*- coding: utf-8 -*-
"""MPL-MIXER.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1665a69VivdQZJB2LYOKwB3YoQYiIPjnS

# MLP-Mixer : An all-MLP architecture for Vision
"""

pip install einops

"""#REQUIRED PACKAGES"""

import einops
import torch
import torch.nn as nn
import torchvision
import torchvision.transforms as transforms
from tqdm import tqdm
from torch.utils.data import DataLoader
from einops.layers.torch import Rearrange
from torchvision import datasets, transforms

"""First we define the MLP block that we will use in the Mixer"""

class MLP(nn.Module):
  """
    dim is the input dimension in the MLP, and hidden_dim is the dimensions of the hidden layers

    """
  def __init__(self, dim, hidden_dim):
    
    super().__init__()
    self.lin1 = nn.Linear(dim, hidden_dim)
    self.gelu = nn.GELU()
    self.lin2 = nn.Linear(hidden_dim, dim)

  def forward(self, x):
    x = self.lin1(x)
    x = self.gelu(x)
    x = self.lin2(x)

    return x

"""We then  define the mixer layer, this will contain two MLP layers using the class we have already defined above with Layer normalisations.

It will be divided into token mixing blocks and channel mixing blocks

"""

class MixerLayer(nn.Module):


  """
  will have multiple parameters include :
  ~n_patches - the number of patches each image will be split into

  ~token_dim - dimension of the hidden layer of the token mixing MLP block

  ~channel_dim - dimensions of the hidden layer of the channel mixing MLP block 

  ~dim - dimensions of patches(number of channels)

  """


  def __init__(self, n_patches, dim, token_dim, channel_dim ):

    super().__init__()

    self.norm1 = nn.LayerNorm(dim)
    self.token_mix = MLP(n_patches, token_dim)
    self.channel_mix = MLP(dim, channel_dim)
    self.norm2 = nn. LayerNorm(dim)

  def forward(self, x):

    y = self.norm1(x)
    y = self.token_mix(torch.transpose(y, 1, 2))
    y = torch.transpose(y, 1, 2) 
    x = x + y
    y = self.norm2(x)
    out = x + self.channel_mix(y)

    return out

"""After we have defined our mixerlayers we then implement the complete MLP-Mixer network

"""
  will have multiple parameters include :
  ~n_blocks - the number of mixer blocks we wish to have

  ~token_dim - dimension of the hidden layer of the token mixing MLP block

  ~channel_dim - dimensions of the hidden layer of the channel mixing MLP block 

  ~dim - dimensions of patches(number of channels)

  ~n_classes - number of classes our classifier will have

  ~ img_sz and patch_sz - size of the image and patch respectively (assume square)
"""

class MLPMixer(nn.Module):
  def __init__(self, img_sz, patch_sz, dim, token_dim, channel_dim, n_classes,n_blocks):
    super().__init__()
    
    n_patches = (img_sz//patch_sz)**2

    #to extract patch from input images
    self.patch_kernel = nn.Conv2d(3, dim, kernel_size = patch_sz, stride= patch_sz)

    #our N mixer blocks
    self.blocks = nn.ModuleList(
        [ 
         MixerLayer(
             n_patches = n_patches,
             dim = dim,
             token_dim = token_dim,
             channel_dim = channel_dim,
         )
         for _ in range(n_blocks)
         ] 
      )
    self.pre_head_norm = nn.LayerNorm(dim)
    self.head_classifier = nn.Linear(dim, n_classes)

  def forward(self, x):

    x = self.patch_kernel(x) #input to mixerlayers is patch in tensor form not image
    x = einops.rearrange(x, "n c h w -> n (h w) c")

    for mixer_block in self.blocks:

      x = mixer_block(x)
      x = self.pre_head_norm(x)
      x = x.mean(dim = 1) #avarage pooling
      y = self.head_classifier(x)

      return y

"""##Experiments"""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
n_epochs = 10
img_sz = 224
token_dim = 312
channel_dim = 512
patch_sz = 16
n_blocks = 4
dim = 256
n_classes = 10
batch_size = 512
lr = 3e-4
T = transforms.Compose(
    [
     transforms.Resize((img_sz, img_sz)),
     transforms.ToTensor()
    ]
)

train_data = datasets.CIFAR10("data/", train=True, download=True, transform=T)
val_data = datasets.CIFAR10("data/", train=False, download=True, transform=T)
train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)
val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)
x, y = next(iter(train_loader))
print(len(train_data), x.shape, y.shape)

net = MLPMixer(img_sz, patch_sz, dim, token_dim, channel_dim, n_classes,n_blocks)

optimizer = torch.optim.Adam(net.parameters(), lr=lr)
loss_fn = nn.CrossEntropyLoss()
def get_accuracy(preds, y):
    preds = preds.argmax(dim=1, keepdim=True)
    correct = preds.squeeze(1).eq(y)
    acc = correct.sum() / torch.FloatTensor([y.shape[0]]).to(device)
    return acc

def loop(net, loader, is_train):
    net.train(is_train)
    losses = []
    accs = []
    pbar = tqdm(loader, total=len(loader))
    for x, y in pbar:
        x = x.to(device)
        y = y.to(device)
        with torch.set_grad_enabled(is_train):
            preds = net(x)
            loss = loss_fn(preds, y)
            acc = get_accuracy(preds, y)
            losses.append(loss.item())
            accs.append(acc.item())
        if is_train:
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        pbar.set_description(f'epoch={epoch}, train={int(is_train)}')
        pbar.set_postfix(loss=f'{np.mean(losses):.4f}', acc=f'{np.mean(accs):.4f}')

for epoch in range(n_epochs):
    loop(net, train_loader, True)
    loop(net, val_loader, False)

